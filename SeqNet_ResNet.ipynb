{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#########################\n",
    "max_length =50\n",
    "number_of_sequence=489348\n",
    "sequence_size=max_length*4+1\n",
    "number_of_element=number_of_sequence*sequence_size\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def make_image(seq):\n",
    "    image=np.zeros(shape=(4,max_length),dtype=np.float16)\n",
    "                 \n",
    "    for i in range(len(seq)):\n",
    "        if seq[i]=='A':\n",
    "            image[0,i]=1\n",
    "        elif seq[i]=='T':\n",
    "            image[1,i]=1\n",
    "        elif seq[i]=='G':\n",
    "             image[2,i]=1\n",
    "        else:\n",
    "            image[3,i]=1\n",
    "                            \n",
    "    return image\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def encode_feature(seq,lable):\n",
    "    s=np.zeros(shape=(201,1),dtype=np.float16)\n",
    "    x_image=make_image(seq) \n",
    "    s[0]=lable\n",
    "    s[1:201]=x_image.reshape(4*max_length,1)\n",
    "    return s\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def Generate_binary(seq_file):\n",
    "    s=np.zeros(shape=(number_of_element,1),dtype=np.float16)\n",
    "    data=pd.read_csv(seq_file)\n",
    "    seq=data['UTR'].values\n",
    "    label=data['growth_rate'].values\n",
    "    start=0\n",
    "    end=sequence_size\n",
    "    for i in range(number_of_sequence):\n",
    "        e=encode_feature(data['UTR'].values[i],data['growth_rate'].values[i])\n",
    "        s[start:end]=encode_feature(data['UTR'].values[i],data['growth_rate'].values[i])\n",
    "        start=start+sequence_size\n",
    "        end=end+sequence_size\n",
    "        if(i%10000==0):\n",
    "            print(i)  \n",
    "    s.tofile(\"s.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n"
     ]
    }
   ],
   "source": [
    "Generate_binary(\"Random_UTRs.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "_HEIGHT = 4\n",
    "_WIDTH = 50\n",
    "_DEPTH = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_DATA_FILES = 1\n",
    "\n",
    "\n",
    "filepath='C:\\\\Users\\\\H P\\\\nn_yeast'\n",
    "filename='s.bin'\n",
    "\n",
    "file_data= [os.path.join(filepath, filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bytes_in_file_data=sum([os.path.getsize(os.path.join(filepath, filename))])\n",
    "record_size=402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [1.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [1.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [1.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [1.]]\n",
      "\n",
      "  [[0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [0.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n"
     ]
    }
   ],
   "source": [
    "def get_filenames(is_training, data_dir):\n",
    "    if is_training:\n",
    "        return [os.path.join(data_dir, 's.bin')]\n",
    "    else:\n",
    "        return [os.path.join(data_dir, 's.bin')]\n",
    "\n",
    "def record_dataset(filenames):\n",
    "  \"\"\"Returns an input pipeline Dataset from `filenames`.\"\"\"\n",
    "  record_bytes = 402\n",
    "  return tf.data.FixedLengthRecordDataset(filenames, record_bytes)\n",
    "\n",
    "def parse_record(raw_record):\n",
    "  \"\"\"Parse CIFAR-10 image and label from a raw record.\"\"\"\n",
    "  # Every record consists of a label followed by the image, with a fixed number\n",
    "  # of bytes for each.\n",
    "  label_bytes = 2\n",
    "  image_bytes = 2*(_HEIGHT * _WIDTH * _DEPTH)\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "\n",
    "  # Convert bytes to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.float16)\n",
    "\n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.float32)\n",
    "  #label = tf.one_hot(label, _NUM_CLASSES)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "      record_vector[1:201], [_DEPTH, _HEIGHT, _WIDTH])\n",
    "\n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(is_training, data_dir, batch_size, num_epochs=1):\n",
    "  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n",
    "\n",
    "  Args:\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    data_dir: The directory containing the input data.\n",
    "    batch_size: The number of samples per batch.\n",
    "    num_epochs: The number of epochs to repeat the dataset.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of images and labels.\n",
    "  \"\"\"\n",
    "  dataset = record_dataset(get_filenames(is_training, data_dir))\n",
    "\n",
    "  if is_training:\n",
    "    # When choosing shuffle buffer sizes, larger sizes result in better\n",
    "    # randomness, while smaller sizes have better performance. Because CIFAR-10\n",
    "    # is a relatively small dataset, we choose to shuffle the full epoch.\n",
    "    dataset = dataset.shuffle(buffer_size=5000)\n",
    "\n",
    "  dataset = dataset.map(parse_record)\n",
    "  \n",
    "  dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "  # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "  # iterator.\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  images, labels = iterator.get_next()\n",
    "\n",
    "  return images, labels\n",
    "\n",
    "(seq,label)=input_fn(1, filepath, 100000, num_epochs=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        seq_example=sess.run(seq)\n",
    "        print(seq_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19857812]\n",
      " [ 0.21863428]\n",
      " [ 0.18737426]\n",
      " ...\n",
      " [-0.01803339]\n",
      " [ 0.37763187]\n",
      " [-0.13725513]] 2.0660594 [ 1.1835938  -0.20300293  0.52001953 ... -1.2275391   0.52001953\n",
      "  0.11206055]\n"
     ]
    }
   ],
   "source": [
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "def batch_norm_relu(inputs, is_training, data_format):\n",
    "  \"\"\"Performs a batch normalization followed by a ReLU.\"\"\"\n",
    "  # We set fused=True for a significant performance boost. See\n",
    "  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n",
    "  inputs = tf.layers.batch_normalization(\n",
    "      inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
    "      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n",
    "      scale=True, training=is_training, fused=True)\n",
    "  inputs = tf.nn.relu(inputs)\n",
    "  return inputs\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n",
    "  \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "  # The padding is consistent and is based only on `kernel_size`, not on the\n",
    "  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n",
    "  if strides > 1:\n",
    "    inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "  return tf.layers.conv2d(\n",
    "      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "      padding='SAME', use_bias=False,\n",
    "      kernel_initializer=tf.variance_scaling_initializer(),\n",
    "      data_format=data_format)\n",
    "\n",
    "def building_block(inputs, filters, is_training, projection_shortcut, strides,\n",
    "                   data_format):\n",
    "  \"\"\"Standard building block for residual networks with BN before convolutions.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the convolutions.\n",
    "    is_training: A Boolean for whether the model is in training or inference\n",
    "      mode. Needed for batch normalization.\n",
    "    projection_shortcut: The function to use for projection shortcuts (typically\n",
    "      a 1x1 convolution when downsampling the input).\n",
    "    strides: The block's stride. If greater than 1, this block will ultimately\n",
    "      downsample the input.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "\n",
    "  Returns:\n",
    "    The output tensor of the block.\n",
    "  \"\"\"\n",
    "  shortcut = inputs\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "\n",
    "  # The projection shortcut should come after the first batch norm and ReLU\n",
    "  # since it performs a 1x1 convolution.\n",
    "  if projection_shortcut is not None:\n",
    "    shortcut = projection_shortcut(inputs)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=2, strides=strides,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=2, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  return inputs + shortcut\n",
    "\n",
    "def block_layer(inputs, filters, block_fn, blocks, strides, is_training, name,\n",
    "                data_format):\n",
    "  \"\"\"Creates one layer of blocks for the ResNet model.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the first convolution of the layer.\n",
    "    block_fn: The block to use within the model, either `building_block` or\n",
    "      `bottleneck_block`.\n",
    "    blocks: The number of blocks contained in the layer.\n",
    "    strides: The stride to use for the first convolution of the layer. If\n",
    "      greater than 1, this layer will ultimately downsample the input.\n",
    "    is_training: Either True or False, whether we are currently training the\n",
    "      model. Needed for batch norm.\n",
    "    name: A string name for the tensor output of the block layer.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "\n",
    "  Returns:\n",
    "    The output tensor of the block layer.\n",
    "  \"\"\"\n",
    "  # Bottleneck blocks end with 4x the number of filters as they start with\n",
    "  filters_out = 4 * filters if block_fn is bottleneck_block else filters\n",
    "\n",
    "  def projection_shortcut(inputs):\n",
    "    return conv2d_fixed_padding(\n",
    "        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "\n",
    "  # Only the first block per block_layer uses projection_shortcut and strides\n",
    "  inputs = block_fn(inputs, filters, is_training, projection_shortcut, strides,\n",
    "                    data_format)\n",
    "\n",
    "\n",
    "  for _ in range(1, blocks):\n",
    "    inputs = block_fn(inputs, filters, is_training, None, 1, data_format)\n",
    "\n",
    "  return tf.identity(inputs, name)\n",
    "\n",
    "def bottleneck_block(inputs, filters, is_training, projection_shortcut,\n",
    "                     strides, data_format):\n",
    "  \"\"\"Bottleneck block variant for residual networks with BN before convolutions.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the first two convolutions. Note that the\n",
    "      third and final convolution will use 4 times as many filters.\n",
    "    is_training: A Boolean for whether the model is in training or inference\n",
    "      mode. Needed for batch normalization.\n",
    "    projection_shortcut: The function to use for projection shortcuts (typically\n",
    "      a 1x1 convolution when downsampling the input).\n",
    "    strides: The block's stride. If greater than 1, this block will ultimately\n",
    "      downsample the input.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "\n",
    "  Returns:\n",
    "    The output tensor of the block.\n",
    "  \"\"\"\n",
    "  shortcut = inputs\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "\n",
    "  # The projection shortcut should come after the first batch norm and ReLU\n",
    "  # since it performs a 1x1 convolution.\n",
    "  if projection_shortcut is not None:\n",
    "    shortcut = projection_shortcut(inputs)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  return inputs + shortcut\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seqnet_resnet(inputs):\n",
    "    seq_data_format =  'channels_last'\n",
    "    num_blocks = 1\n",
    "    is_training=True\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs=inputs, filters=4, kernel_size=3, strides=1,data_format=seq_data_format)\n",
    "    inputs = tf.identity(inputs, 'initial_conv')\n",
    "\n",
    "    inputs = block_layer(inputs=inputs, filters=4, block_fn=building_block, blocks=num_blocks,\n",
    "             strides=1, is_training=is_training, name='block_layer1',\n",
    "             data_format=seq_data_format)\n",
    "\n",
    "    inputs = block_layer(inputs=inputs, filters=4, block_fn=building_block, blocks=num_blocks,\n",
    "             strides=1, is_training=is_training, name='block_layer2',\n",
    "             data_format=seq_data_format)\n",
    "\n",
    "    inputs = block_layer(inputs=inputs, filters=4, block_fn=building_block, blocks=num_blocks,\n",
    "             strides=1, is_training=is_training, name='block_layer3',\n",
    "             data_format=seq_data_format)\n",
    "\n",
    "    inputs = batch_norm_relu(inputs, is_training,data_format=seq_data_format)\n",
    "\n",
    "    inputs = tf.layers.average_pooling2d(\n",
    "            inputs=inputs, pool_size=3, strides=1, padding='VALID',\n",
    "            data_format=seq_data_format)\n",
    "    inputs = tf.identity(inputs, 'final_avg_pool')\n",
    "\n",
    "    inputs = tf.reshape(inputs, [-1, 6*64])\n",
    "\n",
    "    inputs = tf.layers.dense(inputs=inputs, units=16)\n",
    "    inputs = tf.layers.dense(inputs=inputs, units=1)\n",
    "\n",
    "\n",
    "    inputs = tf.identity(inputs, 'final_dense')\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "network = seqnet_resnet(seq)\n",
    "labelT=tf.reshape(label,[-1,1])\n",
    "\n",
    "loss = tf.losses.mean_squared_error(predictions=network, labels=labelT)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        L1_output=sess.run(tf.global_variables_initializer())\n",
    "        Network=sess.run(network)\n",
    "        Loss=sess.run(loss)\n",
    "        Label=sess.run(label)\n",
    "\n",
    "\n",
    "        print(Network,Loss,Label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNlab",
   "language": "python",
   "name": "nnlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
